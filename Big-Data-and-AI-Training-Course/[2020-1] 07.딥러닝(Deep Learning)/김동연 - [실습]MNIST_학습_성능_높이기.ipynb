{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST 학습 성능 높이기.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nbsbohTMp-Zh",
        "itBjeGS-vQi-",
        "eVKs6vDIwCZN",
        "mgkGwvZewGE2",
        "efZ5-0QIwF7y",
        "jq_Umt-qwOLx",
        "fm_FInuUwQUQ",
        "PSArAOBAzZ8H",
        "7zcprwHBzBPn",
        "MUO5dDGtwa62",
        "BIjbtanh10Mt",
        "Dzo5ORwov08N",
        "w-oLBMkOuD-t",
        "5i_6R4BZuD-7",
        "p0cSegcV0mJC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbsbohTMp-Zh",
        "colab_type": "text"
      },
      "source": [
        "## Momentum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2iZYspZpv9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Momentum:\n",
        "\n",
        "  def __init__(self, learning_rate = 0.01, momentum=0.9):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.momentum = momentum\n",
        "    self.v = None\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    if self.v is None:\n",
        "      self.v = {}\n",
        "      for key, val in params.items():\n",
        "        self.v[key] = np.zeros_like(val)\n",
        "    \n",
        "    for key in params.keys():\n",
        "      self.v[key]  = self.momentum * self.v[key] - self.learning_rate * grads[key]\n",
        "      params[key] += self.v[key]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itBjeGS-vQi-",
        "colab_type": "text"
      },
      "source": [
        "## MNIST 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgz8_GjSveBi",
        "colab_type": "text"
      },
      "source": [
        "### Modules Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p9ArEUkInYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dg7-vr1vj1_",
        "colab_type": "text"
      },
      "source": [
        "### 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu_5fUTVvfyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "num_classes = 10"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCILIUjovmhj",
        "colab_type": "text"
      },
      "source": [
        "### 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZX6J11WvgsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "num_classes = 10\n",
        "\n",
        "x_train = x_train[:10000]\n",
        "x_test = x_test[:3000]\n",
        "\n",
        "y_train = y_train[:10000]\n",
        "y_test = y_test[:3000]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqckLwhYvgwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test = x_train.reshape(-1,28*28).astype(np.float32), x_test.reshape(-1,28*28).astype(np.float32)\n",
        "\n",
        "x_train = x_train / .255\n",
        "x_test = x_test / .255\n",
        "\n",
        "y_train = np.eye(num_classes)[y_train]"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwwZGRjbtXDy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "c9718404-543e-430f-941d-4e82db6452ff"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 784)\n",
            "(10000, 10)\n",
            "(3000, 784)\n",
            "(3000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXAz-S0Nv0gn",
        "colab_type": "text"
      },
      "source": [
        "### Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maUj-a-5vg5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 1000\n",
        "learning_rate = 1e-2\n",
        "batch_size = 256\n",
        "train_size = x_train.shape[0]\n",
        "iter_pre_epoch = max(train_size / batch_size, 1)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVKs6vDIwCZN",
        "colab_type": "text"
      },
      "source": [
        "### Util Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVYcEN1pvhWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "  if x.ndim == 2:\n",
        "    x = x.T\n",
        "    x = x - np.max(x,axis=0)\n",
        "    y = np.exp(x) / np.sum(np.exp(x), axis =0)\n",
        "    return y.T\n",
        "\n",
        "  x = x - np.max(x)\n",
        "  return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "  return .5 * np.sum((y-t)**2)\n",
        "\n",
        "def cross_entropy_error(pred_y, true_y):\n",
        "  if pred_y.ndim == 1:\n",
        "    true_y = true_y.reshape(1, true_y.size)\n",
        "    pred_y = pred_y.reshape(1, pred_y.size)\n",
        "\n",
        "  if true_y.size == pred_y.size:\n",
        "    true_y = true_y.argmax(axis=1)\n",
        "\n",
        "  batch_size = pred_y.shape[0]\n",
        "  return -np.sum(np.log(pred_y[np.arange(batch_size), true_y] + 1e-7)) / batch_size"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgkGwvZewGE2",
        "colab_type": "text"
      },
      "source": [
        "### Util Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efZ5-0QIwF7y",
        "colab_type": "text"
      },
      "source": [
        "#### ReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUN_eIfjvhc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReLU:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, input_data):\n",
        "    self.mask = (input_data <= 0)\n",
        "    out = input_data.copy()\n",
        "    out[self.mask] = 0\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout[self.mask] = 0\n",
        "    dx = dout\n",
        "\n",
        "    return dx"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq_Umt-qwOLx",
        "colab_type": "text"
      },
      "source": [
        "#### Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LdQiHGHvha7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self, input_data):\n",
        "    out = 1 / (1.0 + np.exp(-input_data))\n",
        "    self.out = out\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out #self.dout 은 오타?\n",
        "    return dx"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm_FInuUwQUQ",
        "colab_type": "text"
      },
      "source": [
        "#### Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OgM39CRvhQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "  def __init__(self,W,b):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "\n",
        "    self.input_data = None\n",
        "    self.input_data_shape = None\n",
        "\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "\n",
        "  def forward(self, input_data):\n",
        "    self.input_data_shape = input_data.shape\n",
        "\n",
        "    input_data = input_data.reshape(input_data.shape[0], -1)\n",
        "    self.input_data = input_data\n",
        "    out = np.dot(self.input_data, self.W) + self.b\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = np.dot(dout, self.W.T)\n",
        "    self.dW = np.dot(self.input_data.T, dout)\n",
        "    self.db = np.sum(dout,axis=0)\n",
        "\n",
        "    dx = dx.reshape(*self.input_data_shape)\n",
        "\n",
        "    return dx"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSArAOBAzZ8H",
        "colab_type": "text"
      },
      "source": [
        "#### Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZwJVo30zbIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchNormalization:\n",
        "\n",
        "  def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var = None):\n",
        "    self.gamma = gamma\n",
        "    self.beta = beta\n",
        "    self.momentum = momentum\n",
        "    self.input_shape = None\n",
        "\n",
        "    self.running_mean = running_mean\n",
        "    self.running_var = running_var\n",
        "\n",
        "    self.batch_size = None\n",
        "    self.xc = None\n",
        "    self.std = None\n",
        "    self.dgamma = None\n",
        "    self.dbeta = None\n",
        "  \n",
        "  def forward(self, input_data, is_train=True):\n",
        "    self.input_shape = input_data.shape\n",
        "    if input_data.ndim != 2:\n",
        "      N, C, H, W = input_data.shape\n",
        "      input_data = input_data.reshape(N,-1)\n",
        "\n",
        "    out = self.__forward(input_data, is_train)\n",
        "\n",
        "    return out.reshape(*self.input_shape)\n",
        "\n",
        "  def __forward(self, input_data, is_train):\n",
        "    if self.running_mean is None:\n",
        "      N, D = input_data.shape\n",
        "      self.running_mean = np.zeros(D)\n",
        "      self.running_var = np.zeros(D)\n",
        "\n",
        "    if is_train:\n",
        "      mu = input_data.mean(axis=0)\n",
        "      xc = input_data - mu\n",
        "      var = np.mean(xc**2, axis=0)\n",
        "      std = np.sqrt(var + 10e-7)\n",
        "      xn = xc / std\n",
        "\n",
        "      self.batch_size = input_data.shape[0]\n",
        "      self.xc = xc\n",
        "      self.xn = xn\n",
        "      self.std = std\n",
        "      self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "      self.running_var = self.momentum * self.running_var + (1-self.momentum) * var\n",
        "    else:\n",
        "      xc = input_data - self.running_mean\n",
        "      xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "\n",
        "    out = self.gamma * xn + self.beta\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    if dout.ndim != 2:\n",
        "      N, C, H, W = dout.shape\n",
        "      dout = dout.reshape(N, -1)\n",
        "\n",
        "    dx = self.__backward(dout)\n",
        "\n",
        "    dx = dx.reshape(*self.input_shape)\n",
        "    return dx\n",
        "\n",
        "  def __backward(self, dout):\n",
        "    dbeta = dout.sum(axis=0)\n",
        "    dgamma = np.sum(self.xn * dout, axis=0)\n",
        "    dxn = self.gamma * dout\n",
        "    dxc = dxn / self.std\n",
        "    dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "    dvar = .5 * dstd / self.std\n",
        "    dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "    dmu = np.sum(dxc, axis=0)\n",
        "    dx = dxc- dmu / self.batch_size\n",
        "\n",
        "    self.dgamma = dgamma\n",
        "    self.dbeta = dbeta\n",
        "    return dx"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zcprwHBzBPn",
        "colab_type": "text"
      },
      "source": [
        "#### Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiHwGZQgzCtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dropout:\n",
        "  def __init__(self, dropout_ratio = .5):\n",
        "    self.dropout_ratio = dropout_ratio\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self,input_data, is_train=True):\n",
        "    if is_train:\n",
        "      self.mask = np.random.rand(*input_data.shape) > self.dropout_ratio\n",
        "      return input_data * self.mask\n",
        "\n",
        "    else:\n",
        "      return input_data * (1.0 - self.dropout_ratio)\n",
        "\n",
        "  def backward(self,dout):\n",
        "    return dout * self.mask"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUO5dDGtwa62",
        "colab_type": "text"
      },
      "source": [
        "#### Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe4RY7pVvhN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Softmax:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.loss = None\n",
        "    self.y = None\n",
        "    self.t = None\n",
        "\n",
        "  def forward(self, input_data, t):\n",
        "    self.t = t\n",
        "    self.y = softmax(input_data)\n",
        "    self.loss = cross_entropy_error(self.y, self.t)\n",
        "    return self.loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    batch_size = self.t.shape[0]\n",
        "\n",
        "    if self.t.size == self.y.size:\n",
        "      dx = (self.y - self.t) / batch_size\n",
        "    else:\n",
        "      dx = self.y.copy()\n",
        "      dx[np.arange(batch_size), self.t] -= 1\n",
        "      dx = dx / batch_size\n",
        "\n",
        "    return dx"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIjbtanh10Mt",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h442ImGrvhMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel:\n",
        "\n",
        "  def __init__(self, input_size, hidden_size_list, output_size,\n",
        "               activation='relu', decay_lambda = 0,\n",
        "               use_dropout=False, dropout_ratio = 0.5, use_batchnorm=False):\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.hidden_size_list = hidden_size_list\n",
        "    self.hidden_layer_num = len(hidden_size_list)\n",
        "    self.use_dropout = use_dropout\n",
        "    self.decay_lambda = decay_lambda\n",
        "    self.use_batchnorm = use_batchnorm\n",
        "    self.params = {}\n",
        "\n",
        "    self.__init_weight(activation)\n",
        "    \n",
        "    activation_layer = {'sigmoid':Sigmoid, 'relu':ReLU}\n",
        "    self.layers = OrderedDict()\n",
        "    for idx in range(1, self.hidden_layer_num+1):\n",
        "      self.layers['Layer' + str(idx)] = Layer(self.params['W' + str(idx)],\n",
        "                                              self.params['b' + str(idx)])\n",
        "      if self.use_batchnorm:\n",
        "        self.params['gamma' + str(idx)] = np.ones(hidden_size_list[idx-1])\n",
        "        self.params['beta' + str(idx)] = np.ones(hidden_size_list[idx-1])\n",
        "        self.layers['BatchNorm' + str(idx)] = BatchNormalization(self.params['gamma' + str(idx)], self.params['beta' + str(idx)])\n",
        "\n",
        "      self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
        "\n",
        "      if self.use_dropout:\n",
        "        self.layers['Dropout' + str(idx)] = Dropout(dropout_ratio)\n",
        "\n",
        "    idx = self.hidden_layer_num + 1\n",
        "    self.layers['Layer' + str(idx)] = Layer(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
        "    self.last_layer = Softmax()\n",
        "\n",
        "  def __init_weight(self, activation):\n",
        "    all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
        "\n",
        "    for idx in range(1, len(all_size_list)):\n",
        "      scale = None\n",
        "      if activation.lower() == 'relu':\n",
        "        scale = np.sqrt(2.0 / all_size_list[idx-1])\n",
        "      elif activation.lower() == 'sigmoid':\n",
        "        scale = np.sqrt(1.0 / all_size_list[idx-1])\n",
        "\n",
        "      self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
        "      self.params['b' + str(idx)] =  np.zeros(all_size_list[idx])\n",
        "\n",
        "    \n",
        "  def predict(self, x, is_train=False):\n",
        "    for key, layer in self.layers.items():\n",
        "      if 'Dropout' in key or 'BatchNorm' in key:\n",
        "        x = layer.forward(x, is_train)\n",
        "      else:\n",
        "        x = layer.forward(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def loss(self, x, t, is_train=False):\n",
        "    y = self.predict(x,is_train)\n",
        "\n",
        "    weight_decay = 0\n",
        "    for idx in range(1, self.hidden_layer_num + 2):\n",
        "      W = self.params['W' + str(idx)]\n",
        "      weight_decay += 0.5 * self.decay_lambda * np.sum(W**2)\n",
        "\n",
        "    return self.last_layer.forward(y,t) + weight_decay\n",
        "\n",
        "  def accuracy(self,x,t):\n",
        "    y = self.predict(x, is_train=False)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    if t.ndim != 1:\n",
        "      t = np.argmax(t,axis=1)\n",
        "    \n",
        "    accuracy = np.sum(y ==t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    self.loss(x, t, is_train=True)\n",
        "\n",
        "    dout = 1\n",
        "    dout = self.last_layer.backward(dout)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    grads = {}\n",
        "    for idx in range(1, self.hidden_layer_num+2):\n",
        "      grads['W' + str(idx)] = self.layers['Layer' + str(idx)].dW + self.decay_lambda * self.params['W' + str(idx)]\n",
        "      grads['b' + str(idx)] = self.layers['Layer' + str(idx)].db\n",
        "\n",
        "      if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
        "        grads['gamma' + str(idx)] = self.layers['BatchNorm' + str(idx)].dgamma\n",
        "        grads['beta' + str(idx)] = self.layers['BatchNorm' + str(idx)].dbeta\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bd7rOvonsHYy"
      },
      "source": [
        "## 모델 생성 및 학습 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vkiL2li7sHYz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "910b8856-44a6-4caf-e374-cdb9775d56d7"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "num_classes = 10\n",
        "\n",
        "x_train = x_train[:10000]\n",
        "x_test = x_test[:3000]\n",
        "\n",
        "y_train = y_train[:10000]\n",
        "y_test = y_test[:3000]\n",
        "\n",
        "x_train, x_test = x_train.reshape(-1,28*28).astype(np.float32), x_test.reshape(-1,28*28).astype(np.float32)\n",
        "\n",
        "x_train = x_train / .255\n",
        "x_test = x_test / .255\n",
        "\n",
        "y_train = np.eye(num_classes)[y_train]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 784)\n",
            "(10000, 10)\n",
            "(3000, 784)\n",
            "(3000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NbEwhpG5sHY2",
        "colab": {}
      },
      "source": [
        "epochs = 1000\n",
        "learning_rate = 1e-3\n",
        "batch_size = 100\n",
        "train_size = x_train.shape[0]\n",
        "iter_pre_epoch = max(train_size / batch_size, 1)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m2fAgERbsHY4",
        "colab": {}
      },
      "source": [
        "decay_lambda_1 = 0.15\n",
        "model_1 = MyModel(input_size=784, hidden_size_list=[100, 100, 100, 100], output_size=10,\n",
        "                  decay_lambda=decay_lambda_1, use_batchnorm=True)\n",
        "optimizer = Momentum(learning_rate=learning_rate)\n",
        "\n",
        "model_1_train_loss_list = []\n",
        "model_1_train_acc_list = []\n",
        "model_1_test_acc_list = []"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WxosF1E9sHY6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "aca4b05c-dfbb-4574-eb1e-225b656a4f11"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  y_batch = y_train[batch_mask]\n",
        "\n",
        "  grads = model_1.gradient(x_batch, y_batch)\n",
        "  optimizer.update(model_1.params, grads)\n",
        "\n",
        "  loss = model_1.loss(x_batch, y_batch)\n",
        "  model_1_train_loss_list.append(loss)\n",
        "\n",
        "  train_acc = model_1.accuracy(x_train, y_train)\n",
        "  test_acc = model_1.accuracy(x_test, y_test)\n",
        "  model_1_train_acc_list.append(train_acc)\n",
        "  model_1_test_acc_list.append(test_acc)\n",
        "\n",
        "  if epoch % 50 == 0:\n",
        "    print('[Model 1] Epoch: {}, Train Loss: {:.4f} Train Accuracy: {:.4f} Test Accuracy: {:.4f}'.format(epoch+1, loss, train_acc, test_acc))"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Model 1] Epoch: 1, Train Loss: 76.7115 Train Accuracy: 0.0832 Test Accuracy: 0.0770\n",
            "[Model 1] Epoch: 51, Train Loss: 55.0378 Train Accuracy: 0.7667 Test Accuracy: 0.7080\n",
            "[Model 1] Epoch: 101, Train Loss: 47.2827 Train Accuracy: 0.8398 Test Accuracy: 0.7797\n",
            "[Model 1] Epoch: 151, Train Loss: 40.5122 Train Accuracy: 0.8738 Test Accuracy: 0.8180\n",
            "[Model 1] Epoch: 201, Train Loss: 34.8000 Train Accuracy: 0.8878 Test Accuracy: 0.8343\n",
            "[Model 1] Epoch: 251, Train Loss: 30.0831 Train Accuracy: 0.9015 Test Accuracy: 0.8483\n",
            "[Model 1] Epoch: 301, Train Loss: 25.8020 Train Accuracy: 0.9121 Test Accuracy: 0.8610\n",
            "[Model 1] Epoch: 351, Train Loss: 22.2532 Train Accuracy: 0.9170 Test Accuracy: 0.8717\n",
            "[Model 1] Epoch: 401, Train Loss: 19.2091 Train Accuracy: 0.9204 Test Accuracy: 0.8807\n",
            "[Model 1] Epoch: 451, Train Loss: 16.5716 Train Accuracy: 0.9253 Test Accuracy: 0.8790\n",
            "[Model 1] Epoch: 501, Train Loss: 14.2565 Train Accuracy: 0.9271 Test Accuracy: 0.8800\n",
            "[Model 1] Epoch: 551, Train Loss: 12.2841 Train Accuracy: 0.9323 Test Accuracy: 0.8900\n",
            "[Model 1] Epoch: 601, Train Loss: 10.6880 Train Accuracy: 0.9367 Test Accuracy: 0.8913\n",
            "[Model 1] Epoch: 651, Train Loss: 9.1373 Train Accuracy: 0.9400 Test Accuracy: 0.8900\n",
            "[Model 1] Epoch: 701, Train Loss: 8.0567 Train Accuracy: 0.9413 Test Accuracy: 0.8903\n",
            "[Model 1] Epoch: 751, Train Loss: 6.8711 Train Accuracy: 0.9446 Test Accuracy: 0.8930\n",
            "[Model 1] Epoch: 801, Train Loss: 5.9370 Train Accuracy: 0.9485 Test Accuracy: 0.8957\n",
            "[Model 1] Epoch: 851, Train Loss: 5.2789 Train Accuracy: 0.9514 Test Accuracy: 0.9040\n",
            "[Model 1] Epoch: 901, Train Loss: 4.4962 Train Accuracy: 0.9512 Test Accuracy: 0.8957\n",
            "[Model 1] Epoch: 951, Train Loss: 3.9143 Train Accuracy: 0.9564 Test Accuracy: 0.9047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dzo5ORwov08N"
      },
      "source": [
        "## 모델 생성 및 학습 2\n",
        " - layer 수를 3개로 뒀으나 1 보다 더 낮음\n",
        "\n",
        "> 들여쓴 블록\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x7wMbumAttDr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "910b8856-44a6-4caf-e374-cdb9775d56d7"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "num_classes = 10\n",
        "\n",
        "x_train = x_train[:10000]\n",
        "x_test = x_test[:3000]\n",
        "\n",
        "y_train = y_train[:10000]\n",
        "y_test = y_test[:3000]\n",
        "\n",
        "x_train, x_test = x_train.reshape(-1,28*28).astype(np.float32), x_test.reshape(-1,28*28).astype(np.float32)\n",
        "\n",
        "x_train = x_train / .255\n",
        "x_test = x_test / .255\n",
        "\n",
        "y_train = np.eye(num_classes)[y_train]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 784)\n",
            "(10000, 10)\n",
            "(3000, 784)\n",
            "(3000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4scLhJk3v08P",
        "colab": {}
      },
      "source": [
        "epochs = 1000\n",
        "learning_rate = 1e-3\n",
        "batch_size = 100\n",
        "train_size = x_train.shape[0]\n",
        "iter_pre_epoch = max(train_size / batch_size, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Ix2HuMev08R",
        "colab": {}
      },
      "source": [
        "decay_lambda_1 = 0.15\n",
        "model_1 = MyModel(input_size=784, hidden_size_list=[100, 100, 100], output_size=10,\n",
        "                  decay_lambda=decay_lambda_1, use_batchnorm=True)\n",
        "optimizer = Momentum(learning_rate=learning_rate)\n",
        "\n",
        "model_1_train_loss_list = []\n",
        "model_1_train_acc_list = []\n",
        "model_1_test_acc_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G1bbCBuRttDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "4ce33da5-a428-4747-bee4-5e3c3d42f4bd"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  y_batch = y_train[batch_mask]\n",
        "\n",
        "  grads = model_1.gradient(x_batch, y_batch)\n",
        "  optimizer.update(model_1.params, grads)\n",
        "\n",
        "  loss = model_1.loss(x_batch, y_batch)\n",
        "  model_1_train_loss_list.append(loss)\n",
        "\n",
        "  train_acc = model_1.accuracy(x_train, y_train)\n",
        "  test_acc = model_1.accuracy(x_test, y_test)\n",
        "  model_1_train_acc_list.append(train_acc)\n",
        "  model_1_test_acc_list.append(test_acc)\n",
        "\n",
        "  if epoch % 50 == 0:\n",
        "    print('[Model 1] Epoch: {}, Train Loss: {:.4f} Train Accuracy: {:.4f} Test Accuracy: {:.4f}'.format(epoch+1, loss, train_acc, test_acc))"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Model 1] Epoch: 1, Train Loss: 74.7462 Train Accuracy: 0.1054 Test Accuracy: 0.1183\n",
            "[Model 1] Epoch: 51, Train Loss: 16.0199 Train Accuracy: 0.8518 Test Accuracy: 0.8087\n",
            "[Model 1] Epoch: 101, Train Loss: 3.3639 Train Accuracy: 0.8774 Test Accuracy: 0.8263\n",
            "[Model 1] Epoch: 151, Train Loss: 1.2234 Train Accuracy: 0.8626 Test Accuracy: 0.8250\n",
            "[Model 1] Epoch: 201, Train Loss: 0.9503 Train Accuracy: 0.8845 Test Accuracy: 0.8417\n",
            "[Model 1] Epoch: 251, Train Loss: 0.6988 Train Accuracy: 0.9024 Test Accuracy: 0.8623\n",
            "[Model 1] Epoch: 301, Train Loss: 0.8320 Train Accuracy: 0.8753 Test Accuracy: 0.8193\n",
            "[Model 1] Epoch: 351, Train Loss: 0.7247 Train Accuracy: 0.8948 Test Accuracy: 0.8453\n",
            "[Model 1] Epoch: 401, Train Loss: 0.9015 Train Accuracy: 0.8626 Test Accuracy: 0.7967\n",
            "[Model 1] Epoch: 451, Train Loss: 0.8360 Train Accuracy: 0.8614 Test Accuracy: 0.7960\n",
            "[Model 1] Epoch: 501, Train Loss: 0.7023 Train Accuracy: 0.9003 Test Accuracy: 0.8627\n",
            "[Model 1] Epoch: 551, Train Loss: 0.7301 Train Accuracy: 0.8943 Test Accuracy: 0.8337\n",
            "[Model 1] Epoch: 601, Train Loss: 0.7178 Train Accuracy: 0.8896 Test Accuracy: 0.8390\n",
            "[Model 1] Epoch: 651, Train Loss: 0.7441 Train Accuracy: 0.9001 Test Accuracy: 0.8450\n",
            "[Model 1] Epoch: 701, Train Loss: 0.7729 Train Accuracy: 0.8639 Test Accuracy: 0.8060\n",
            "[Model 1] Epoch: 751, Train Loss: 0.7961 Train Accuracy: 0.8772 Test Accuracy: 0.8407\n",
            "[Model 1] Epoch: 801, Train Loss: 0.7089 Train Accuracy: 0.8633 Test Accuracy: 0.8220\n",
            "[Model 1] Epoch: 851, Train Loss: 0.7420 Train Accuracy: 0.8899 Test Accuracy: 0.8483\n",
            "[Model 1] Epoch: 901, Train Loss: 0.7222 Train Accuracy: 0.8933 Test Accuracy: 0.8363\n",
            "[Model 1] Epoch: 951, Train Loss: 0.6806 Train Accuracy: 0.8865 Test Accuracy: 0.8143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w-oLBMkOuD-t"
      },
      "source": [
        "## 모델 생성 및 학습 3\n",
        " - learning_rate를 1e-2로 뒀으나 1보다 더 낮음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xkcBQCfduD-y",
        "colab": {}
      },
      "source": [
        "epochs = 1000\n",
        "learning_rate = 1e-2\n",
        "batch_size = 100\n",
        "train_size = x_train.shape[0]\n",
        "iter_pre_epoch = max(train_size / batch_size, 1)"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i5L4g-OHuD-0",
        "colab": {}
      },
      "source": [
        "decay_lambda_1 = 0.15\n",
        "model_1 = MyModel(input_size=784, hidden_size_list=[100, 100, 100, 100], output_size=10,\n",
        "                  decay_lambda=decay_lambda_1, use_batchnorm=True)\n",
        "optimizer = Momentum(learning_rate=learning_rate)\n",
        "\n",
        "model_1_train_loss_list = []\n",
        "model_1_train_acc_list = []\n",
        "model_1_test_acc_list = []"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TdCM1sqYuD-1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "af460da7-0bd6-4f37-fb82-7b0faaa6a147"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  y_batch = y_train[batch_mask]\n",
        "\n",
        "  grads = model_1.gradient(x_batch, y_batch)\n",
        "  optimizer.update(model_1.params, grads)\n",
        "\n",
        "  loss = model_1.loss(x_batch, y_batch)\n",
        "  model_1_train_loss_list.append(loss)\n",
        "\n",
        "  train_acc = model_1.accuracy(x_train, y_train)\n",
        "  test_acc = model_1.accuracy(x_test, y_test)\n",
        "  model_1_train_acc_list.append(train_acc)\n",
        "  model_1_test_acc_list.append(test_acc)\n",
        "\n",
        "  if epoch % 50 == 0:\n",
        "    print('[Model 1] Epoch: {}, Train Loss: {:.4f} Train Accuracy: {:.4f} Test Accuracy: {:.4f}'.format(epoch+1, loss, train_acc, test_acc))"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Model 1] Epoch: 1, Train Loss: 76.6410 Train Accuracy: 0.0711 Test Accuracy: 0.0810\n",
            "[Model 1] Epoch: 51, Train Loss: 16.1280 Train Accuracy: 0.8647 Test Accuracy: 0.8140\n",
            "[Model 1] Epoch: 101, Train Loss: 3.3932 Train Accuracy: 0.8813 Test Accuracy: 0.8350\n",
            "[Model 1] Epoch: 151, Train Loss: 1.3065 Train Accuracy: 0.8700 Test Accuracy: 0.8037\n",
            "[Model 1] Epoch: 201, Train Loss: 0.9624 Train Accuracy: 0.8917 Test Accuracy: 0.8293\n",
            "[Model 1] Epoch: 251, Train Loss: 0.7619 Train Accuracy: 0.8583 Test Accuracy: 0.8060\n",
            "[Model 1] Epoch: 301, Train Loss: 0.7248 Train Accuracy: 0.8858 Test Accuracy: 0.8247\n",
            "[Model 1] Epoch: 351, Train Loss: 0.7672 Train Accuracy: 0.8815 Test Accuracy: 0.8213\n",
            "[Model 1] Epoch: 401, Train Loss: 0.8193 Train Accuracy: 0.8793 Test Accuracy: 0.8167\n",
            "[Model 1] Epoch: 451, Train Loss: 0.7400 Train Accuracy: 0.8813 Test Accuracy: 0.8420\n",
            "[Model 1] Epoch: 501, Train Loss: 0.6393 Train Accuracy: 0.8904 Test Accuracy: 0.8450\n",
            "[Model 1] Epoch: 551, Train Loss: 0.7760 Train Accuracy: 0.8793 Test Accuracy: 0.8297\n",
            "[Model 1] Epoch: 601, Train Loss: 0.8414 Train Accuracy: 0.8590 Test Accuracy: 0.8053\n",
            "[Model 1] Epoch: 651, Train Loss: 0.6860 Train Accuracy: 0.9132 Test Accuracy: 0.8547\n",
            "[Model 1] Epoch: 701, Train Loss: 0.7177 Train Accuracy: 0.8799 Test Accuracy: 0.8463\n",
            "[Model 1] Epoch: 751, Train Loss: 0.7661 Train Accuracy: 0.8913 Test Accuracy: 0.8480\n",
            "[Model 1] Epoch: 801, Train Loss: 0.8114 Train Accuracy: 0.8501 Test Accuracy: 0.7873\n",
            "[Model 1] Epoch: 851, Train Loss: 0.8850 Train Accuracy: 0.8488 Test Accuracy: 0.8047\n",
            "[Model 1] Epoch: 901, Train Loss: 0.6384 Train Accuracy: 0.9122 Test Accuracy: 0.8647\n",
            "[Model 1] Epoch: 951, Train Loss: 0.6384 Train Accuracy: 0.9047 Test Accuracy: 0.8500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5i_6R4BZuD-7"
      },
      "source": [
        "## 모델 생성 및 학습 4\n",
        " - decay_lambda를 0.75로 낮췄으나 1보다 낮음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A1vP0j4AuD--",
        "colab": {}
      },
      "source": [
        "epochs = 1000\n",
        "learning_rate = 1e-3\n",
        "batch_size = 100\n",
        "train_size = x_train.shape[0]\n",
        "iter_pre_epoch = max(train_size / batch_size, 1)"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dED4285LuD_A",
        "colab": {}
      },
      "source": [
        "decay_lambda_1 = 0.75\n",
        "model_1 = MyModel(input_size=784, hidden_size_list=[100, 100, 100,100], output_size=10,\n",
        "                  decay_lambda=decay_lambda_1, use_batchnorm=True)\n",
        "optimizer = Momentum(learning_rate=learning_rate)\n",
        "\n",
        "model_1_train_loss_list = []\n",
        "model_1_train_acc_list = []\n",
        "model_1_test_acc_list = []"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v4NIfF5SuD_B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "2fabffe6-7759-499e-f91c-7547e1229891"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  y_batch = y_train[batch_mask]\n",
        "\n",
        "  grads = model_1.gradient(x_batch, y_batch)\n",
        "  optimizer.update(model_1.params, grads)\n",
        "\n",
        "  loss = model_1.loss(x_batch, y_batch)\n",
        "  model_1_train_loss_list.append(loss)\n",
        "\n",
        "  train_acc = model_1.accuracy(x_train, y_train)\n",
        "  test_acc = model_1.accuracy(x_test, y_test)\n",
        "  model_1_train_acc_list.append(train_acc)\n",
        "  model_1_test_acc_list.append(test_acc)\n",
        "\n",
        "  if epoch % 50 == 0:\n",
        "    print('[Model 1] Epoch: {}, Train Loss: {:.4f} Train Accuracy: {:.4f} Test Accuracy: {:.4f}'.format(epoch+1, loss, train_acc, test_acc))"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Model 1] Epoch: 1, Train Loss: 320.2815 Train Accuracy: 0.0848 Test Accuracy: 0.0903\n",
            "[Model 1] Epoch: 51, Train Loss: 158.6863 Train Accuracy: 0.7467 Test Accuracy: 0.6747\n",
            "[Model 1] Epoch: 101, Train Loss: 71.1839 Train Accuracy: 0.8385 Test Accuracy: 0.7877\n",
            "[Model 1] Epoch: 151, Train Loss: 32.2155 Train Accuracy: 0.8801 Test Accuracy: 0.8340\n",
            "[Model 1] Epoch: 201, Train Loss: 14.9866 Train Accuracy: 0.8968 Test Accuracy: 0.8547\n",
            "[Model 1] Epoch: 251, Train Loss: 7.2714 Train Accuracy: 0.9053 Test Accuracy: 0.8617\n",
            "[Model 1] Epoch: 301, Train Loss: 3.8350 Train Accuracy: 0.9094 Test Accuracy: 0.8623\n",
            "[Model 1] Epoch: 351, Train Loss: 2.3547 Train Accuracy: 0.9258 Test Accuracy: 0.8753\n",
            "[Model 1] Epoch: 401, Train Loss: 1.6755 Train Accuracy: 0.9358 Test Accuracy: 0.8923\n",
            "[Model 1] Epoch: 451, Train Loss: 1.2601 Train Accuracy: 0.9290 Test Accuracy: 0.8827\n",
            "[Model 1] Epoch: 501, Train Loss: 1.2773 Train Accuracy: 0.9354 Test Accuracy: 0.8933\n",
            "[Model 1] Epoch: 551, Train Loss: 1.0331 Train Accuracy: 0.9382 Test Accuracy: 0.8913\n",
            "[Model 1] Epoch: 601, Train Loss: 0.9703 Train Accuracy: 0.9441 Test Accuracy: 0.8983\n",
            "[Model 1] Epoch: 651, Train Loss: 0.9835 Train Accuracy: 0.9285 Test Accuracy: 0.8750\n",
            "[Model 1] Epoch: 701, Train Loss: 0.9801 Train Accuracy: 0.9225 Test Accuracy: 0.8847\n",
            "[Model 1] Epoch: 751, Train Loss: 0.9589 Train Accuracy: 0.9367 Test Accuracy: 0.8900\n",
            "[Model 1] Epoch: 801, Train Loss: 1.0656 Train Accuracy: 0.9337 Test Accuracy: 0.8900\n",
            "[Model 1] Epoch: 851, Train Loss: 0.9286 Train Accuracy: 0.9424 Test Accuracy: 0.8873\n",
            "[Model 1] Epoch: 901, Train Loss: 1.0781 Train Accuracy: 0.9188 Test Accuracy: 0.8753\n",
            "[Model 1] Epoch: 951, Train Loss: 1.0090 Train Accuracy: 0.9327 Test Accuracy: 0.8850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p0cSegcV0mJC"
      },
      "source": [
        "## 모델 생성 및 학습 5\n",
        " - train의 수를 6000으로 늘렸으나 1보다 낮음 결론적으로 1의 성능이 제일 뛰어남"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX6i6MVn0qno",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "22fb9a01-a583-411d-f202-2ffb544e4148"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "num_classes = 10\n",
        "\n",
        "x_train = x_train[:60000]\n",
        "x_test = x_test[:3000]\n",
        "\n",
        "y_train = y_train[:60000]\n",
        "y_test = y_test[:3000]\n",
        "\n",
        "x_train, x_test = x_train.reshape(-1,28*28).astype(np.float32), x_test.reshape(-1,28*28).astype(np.float32)\n",
        "\n",
        "x_train = x_train / .255\n",
        "x_test = x_test / .255\n",
        "\n",
        "y_train = np.eye(num_classes)[y_train]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(60000, 10)\n",
            "(3000, 784)\n",
            "(3000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ljHO_1S0mJD",
        "colab": {}
      },
      "source": [
        "epochs = 1000\n",
        "learning_rate = 1e-3\n",
        "batch_size = 100\n",
        "train_size = x_train.shape[0]\n",
        "iter_pre_epoch = max(train_size / batch_size, 1)"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aPJX_bfr0mJH",
        "colab": {}
      },
      "source": [
        "decay_lambda_1 = 1.5\n",
        "model_1 = MyModel(input_size=784, hidden_size_list=[100, 100, 100,100], output_size=10,\n",
        "                  decay_lambda=decay_lambda_1, use_batchnorm=True)\n",
        "optimizer = Momentum(learning_rate=learning_rate)\n",
        "\n",
        "model_1_train_loss_list = []\n",
        "model_1_train_acc_list = []\n",
        "model_1_test_acc_list = []"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cFPP1MFE0mJJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "62c27bb2-6ed3-4c56-dc4c-17bd89123aa3"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  y_batch = y_train[batch_mask]\n",
        "\n",
        "  grads = model_1.gradient(x_batch, y_batch)\n",
        "  optimizer.update(model_1.params, grads)\n",
        "\n",
        "  loss = model_1.loss(x_batch, y_batch)\n",
        "  model_1_train_loss_list.append(loss)\n",
        "\n",
        "  train_acc = model_1.accuracy(x_train, y_train)\n",
        "  test_acc = model_1.accuracy(x_test, y_test)\n",
        "  model_1_train_acc_list.append(train_acc)\n",
        "  model_1_test_acc_list.append(test_acc)\n",
        "\n",
        "  if epoch % 50 == 0:\n",
        "    print('[Model 1] Epoch: {}, Train Loss: {:.4f} Train Accuracy: {:.4f} Test Accuracy: {:.4f}'.format(epoch+1, loss, train_acc, test_acc))"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Model 1] Epoch: 1, Train Loss: 627.7445 Train Accuracy: 0.0769 Test Accuracy: 0.0710\n",
            "[Model 1] Epoch: 51, Train Loss: 151.3856 Train Accuracy: 0.7484 Test Accuracy: 0.7027\n",
            "[Model 1] Epoch: 101, Train Loss: 26.4802 Train Accuracy: 0.8286 Test Accuracy: 0.7927\n",
            "[Model 1] Epoch: 151, Train Loss: 5.8513 Train Accuracy: 0.8639 Test Accuracy: 0.8290\n",
            "[Model 1] Epoch: 201, Train Loss: 2.4541 Train Accuracy: 0.8525 Test Accuracy: 0.7993\n",
            "[Model 1] Epoch: 251, Train Loss: 1.6172 Train Accuracy: 0.8935 Test Accuracy: 0.8640\n",
            "[Model 1] Epoch: 301, Train Loss: 1.3931 Train Accuracy: 0.8923 Test Accuracy: 0.8630\n",
            "[Model 1] Epoch: 351, Train Loss: 1.4730 Train Accuracy: 0.8371 Test Accuracy: 0.8023\n",
            "[Model 1] Epoch: 401, Train Loss: 1.4562 Train Accuracy: 0.9004 Test Accuracy: 0.8723\n",
            "[Model 1] Epoch: 451, Train Loss: 1.2692 Train Accuracy: 0.8958 Test Accuracy: 0.8653\n",
            "[Model 1] Epoch: 501, Train Loss: 1.3192 Train Accuracy: 0.9076 Test Accuracy: 0.8840\n",
            "[Model 1] Epoch: 551, Train Loss: 1.3763 Train Accuracy: 0.9058 Test Accuracy: 0.8820\n",
            "[Model 1] Epoch: 601, Train Loss: 1.4110 Train Accuracy: 0.8521 Test Accuracy: 0.8123\n",
            "[Model 1] Epoch: 651, Train Loss: 1.4732 Train Accuracy: 0.8698 Test Accuracy: 0.8370\n",
            "[Model 1] Epoch: 701, Train Loss: 1.3810 Train Accuracy: 0.8888 Test Accuracy: 0.8530\n",
            "[Model 1] Epoch: 751, Train Loss: 1.4225 Train Accuracy: 0.8940 Test Accuracy: 0.8700\n",
            "[Model 1] Epoch: 801, Train Loss: 1.4027 Train Accuracy: 0.8844 Test Accuracy: 0.8643\n",
            "[Model 1] Epoch: 851, Train Loss: 1.2820 Train Accuracy: 0.8943 Test Accuracy: 0.8563\n",
            "[Model 1] Epoch: 901, Train Loss: 1.2598 Train Accuracy: 0.9082 Test Accuracy: 0.8790\n",
            "[Model 1] Epoch: 951, Train Loss: 1.3648 Train Accuracy: 0.8962 Test Accuracy: 0.8730\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}