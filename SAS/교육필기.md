sas.com -> 무료체험 설치가능 

---
인메모리 - 빅데이터 처리가 가능하게 데이터 처리가 가능하도록 만들어진 환경

SAS가 오픈소스와 비교한 이점?
-> SAS가 오픈소스인 Python, R 보다 새로운 알고리즘을 받는 건 바로바로 적용은 안되지만 검증된 알고리즘을 사용하고 좋은 이점은 개발부터 운영까지 지원하기에 회사에서는 좋다. 또한 안에서 SAS 언어를 모르더라고 오픈소스 환경까지 지원

데이터 핸들링 - 전처리 과정

관광빅데이터 공모전을 통해 데이터 핸들링 경험

SAS Data Studio vs Visual Analytics
물리적으로 데이터 저장 vs 리포트상 데이터 저장 물리적으로 저장 x

박스플랏
박스 1사분위수 3사분위수
상위 수염 
하위 수염 IQR 상자의 폭에 1.5배를 곱해서 
이 수염을 벗어나는 점을 이상점

샘플사이즈 임팩트
샘플의 사이즈가 커지면 p-value의 값이 작아 질 수 밖에 없어 유의미한 판단은 따로 해야한다.
SAS Cloud Analytics Services(CAS)
SAS Studio
SAS Visual Analytics(VA) 
SAS visual Studio(VS)
SAS Visual Daa Mining and MAchine Learning

SAS BASE : 모든 것이 코드 기반
SAS Emterprise Guide : VA와 같이 코드가 나옴
SAS Studio : 
라이브러리 WORK는 SAS 종료시 사라지고 그 위에 것들은 기본적으로 존재
snippet 코드 예제들



-------------------------------------------------------------------------------------------------------------------------------------------------

one sample t-test 일 표본 
two sample t-test 두 표본
pairwise t-test 쌍체검정 한 사람의 전과 후에 대한 검정 한 대상에대해서 before after를
t-test로 세개 이상의 집단을 비교하면 1종오류가 증가
but ANOVA 검정을 통해 Tukey 방법등의 비교를 사용하면 1종 오류를 높이지 않으면서 비교가 가능하다

-------------------------------------------------------------------------------------------------------------------------------------------------

Dependent Variable: BulbWt

Source	DF	Sum of Squares	Mean Square	F Value	Pr > F
Model	3	0.00457996	0.00152665	1.96	0.1432
Error	28	0.02183054	0.00077966	 	 
Corrected Total	31	0.02641050	 	 	 

SSM과 SSE값 설명할 수 있는 모델의 부분이 0.004 모델로 설명할 수 없는 부분이 0.02로 더 크므로 좋은 모델은 아니다
차이가 없다.

-------------------------------------------------------------------------------------------------------------------------------------------------

Levene's Test for Homogeneity of BulbWt Variance
ANOVA of Squared Deviations from Group Means
Source	DF	Sum of Squares	Mean Square	F Value	Pr > F
Fertilizer	3	1.716E-6	5.719E-7	0.98	0.4173
Error	28	0.000016	5.849E-7	 	 
등분산성
-------------------------------------------------------------------------------------------------------------------------------------------------

다원 ANOVA 변수 여러가지 통제
Dependent Variable: BulbWt

Source	DF	Sum of Squares	Mean Square	F Value	Pr > F
Model	10	0.02307263	0.00230726	5.86	0.0003
Error	21	0.00826745	0.00039369	 	 
Corrected Total	31	0.03134008	 	 	 
설명할 수 있는 부분이 더 높다. -> 시험이 제대로 되었고 R-Square 증가 0.7 넘어가면 좋은 수준
-> 차이가 있다. 어디에서 차이가 났을까? 그래프 확인가능

바이너리(이진) 로지스틱이 머신러닝 보다 금융권에서 많이 사용 됨
-> 그 이유는 설명력이 강하기에 머신러닝 형태는 금감원에서 2순위로 설명력이 부족한 블랙박스가 많다
, 하지만 로지스틱은 명확한 설명력이 존재

-------------------------------------------------------------------------------------------------------------------------------------------------

설명변수가 바이너리 형태면 직접적인 선형을 그릴 수 없음 그래서 y(1) 일 화률과 x간의 관계를 찾음 -> sigmoid
-> logit(p) = Ln(p/(1-p)) = B0 +B1*X -> e^B1 = odds ratio = 오즈비 3.5 여성은 남성에 비해서 구매성형이 3.5배 높다 등
-> 최종 output 확률 p < 1

income  level 저 중 고 원핫인코딩? 
1 0 0
0 1 0
0 0 1
-> logit(p) = B0 + B2*저 + B3*중 + B4*고 변수가 많으면 overfitting 이 일어 날 수 있음 그러므로 B4를 제거 -> 참조코딩
   D저 D 중
저 1	 0  B0 +B1 
중 0	 1  B0 +B2
고 0	 0  B0

Gender	Female	1	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 
 	Male	0	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 
Income	High	1	0	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 
 	Low	0	1	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 
 	Medium	0	0

y = ax + b =일반선형
logit(p) = 일반화된 선형 모형(연결함수)

-------------------------------------------------------------------------------------------------------------------------------------------------

Odds Ratio Estimates
Effect	Point Estimate	95% Wald
Confidence Limits
Gender Female vs Male	1.635	1.046	2.556
Income High vs Medium	2.166	1.294	3.625
Income Low vs Medium	0.973	0.557	1.698

-> 여성이 남성보다 1.6배 구매할 확률이 더 높다
-> 고 소득이 중 소득보다 2.1배 구매할 확률이 더 높다.
-> Odds : 가능성(p/(1-p))  
한 그룹에서의 event 발생비율이 또 다른 그룹에서의 발생 비율보다
상대적으로 얼마나 많이 일어나는지를 나타냄
(ex) 여자가 남자에 비해 100달러 이상 구매할 확률이 얼마나 많은가?

Association of Predicted Probabilities and Observed Responses
Percent Concordant	70.8	Somers' D	0.425
Percent Discordant	28.3	Gamma	0.429
Percent Tied	0.8	Tau-a	0.200
Pairs	43578	c	0.713

-> 모형으로 과거의 것들을 비교 해봤을 떄 Concordant는 예측과 같을 떄 Discordant 예측과 다를 떄

-------------------------------------------------------------------------------------------------------------------------------------------------
LG에서 프로젝트로 보이스를  STT(speach to talk)로 바꾸고 그걸 파이선으로 어느 회사의 고객이었는 지를 분류하는 프로젝트가
있었는데 3개월에 걸친거 버전을 업데이트함에 따른 오류가 심했지만 SAS는 비쥬얼작업으로 간단하게 적용하여 반나절만에
진행이 가능했고 구버전까지 커버가 가능했음.

교호작용 - 새로운 데이터 항목을 만들어야함

분할 -> 데이터 파티션  train, validation(검증)
Hadoop
데이터 게더링이 매우 많은 시간이 걸림 현직자의 수첩 또는 사람의 감, 등 여러 변수들 심지어 데이터 축적이 안되는 경우도 있고
보이스로 되어있어 stt를 개발하여 얻는 기술도 필요 -> 브레인 스토밍을 통해 많은 걸, 데이터마이닝의 기본
머신러닝 -> 데이터마이님 외의 머신이 새로만듦 -> 데이터마이닝은 사람이 이해가 가능 but 머신러닝은 설명은 못해도 잘 맞추면 되지라는 관점으로 바라보고 사용
 overfitting 과적합 다중공산성

의사결정나무 -> 하나하나 컴퓨터가 계산을 하므로 컴퓨팅 파워가 많이들어감
SAS에서는 의사결정나무를 직접 만들 수 있는 모드가 있음 아래쪽 가지를 임의로 자를 수 있고 의사결정나무 트리를 여러개 만들 필요 없고 위에
만들 수 있고 interactive tool 이 있음

모델 중 오분류비율은 계속 떨어지지만 검증용ㅇ 오분류 비율은 떨어지다가 증가 -> 최량 트리 예 = 최적의 모델 선택

auto ML 분석가들이 앞과 뒤의 내용을 연결할 필요성이 있음
데이터를 구성하는 가설 이나 운영및 관리 등 비지니스적인 관점에서 연결 짓는 것을 분석가가 해야할 일
industrial에 관한 지식이 필요 tool을 통해 인더스트리 지식을 좀 갖출 필요가 있음

텍스트 마이닝 -> 새로운 인풋을 만드는 과정이라 생각, 정형데이터 뿐 아니라 비정형데이터를  추가적으로 생성하기에

앙상블 -> 두 모델을 합침(?)
랜덤 포레스트, 그래디언트 부스팅 -> 트리들의 앙상블 Ensenble of decision trees => Bagging tree
랜덤 포레스트 각 트리마다 과적합이 일어나면 모델이 좋아짐 여러 상황을 겪어봄으로서 현재 데이터에 적합한 모델 생성 만약
다 비슷한 트리가 나오면 트리 하나만 만들면 되니 모델의 효율이 떨어짐

Out of bag 

Ramdom Forest -> 첫 번째 트리와 두번 째 트리는 연관성이 없음
Boosting -> 오분류한 것에 가중치를 두어 분류를 진행하므로 첫 번째 트리와 두번째 트리의 연관성이 있음 -> 비선형문제에
대하여 잘 됨

train validate검증 test

모델 생성 -> 파이프라인
입력변수 별 타겟 교차 테이블 -> 어떤 변수가 영향을 미치는 지 쉽게 볼 수 있음 탐색노드에서 가장 중요하다고 생각 됨

데이터가 커서 모델 실행 시간이 오래걸리면 모델을 실행시키는 과정을 보지는 못하는건가?
모델 학습에 있어 오픈 소스인 파이선은 학습 결과를 실시간으로 표기해주는데 SAS는 그러지 못한다.
엔터프라이즈? 로 보면 실행과정을 볼 수 있고, 환경설정에서 메모리 사용량을 봐서 여부 파악이 가능

서버 관리자로 들어가면 -> 레포트 등 내보내기 기능이 있음.

자동수행 -> auto ML 모델에 대한 하이퍼파라미터를 자동 설정

모델 생성 후 실무에 활용 하는 방법??(실무 어플리케이션에 적용 시키는 방법?)
어 떤 모델을 운영을 할지 선택 -> 하위 노드 데이터 스코어링 ->  y hat 이 나옴

모델 구성자? -> 최적의 할당으로 여러 모델 유형에 대한 하이퍼파라미터를 동시에 자동 조율한 다음 최상위 모델을 선택합니다.

홀드아웃 데이터 스코어링 -> 다른 데이터를 통해 스코어를 매기는 것